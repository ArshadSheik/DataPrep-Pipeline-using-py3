{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323888f1",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115de60",
   "metadata": {},
   "source": [
    "### Cleaning & preparing raw-data to use in the data analysis and data science tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0a173",
   "metadata": {},
   "source": [
    "A data preprocessing pipeline is a systematic and automated approach that combines multiple preprocessing steps into a cohesive workflow. It serves as a roadmap for data professionals, guiding them through the transformations and calculations needed to cleanse and prepare data for analysis. The pipeline consists of interconnected steps, each of which is responsible for a specific preprocessing task, such as:\n",
    "\n",
    "1. imputing missing values\n",
    "2. scaling numeric features\n",
    "3. finding and removing outliers\n",
    "4. or encoding categorical variables\n",
    "\n",
    "By following the predefined sequence of operations, the pipeline ensures consistency, reproducibility, and efficiency in overall preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8db847",
   "metadata": {},
   "source": [
    "## The pipeline simply automates the Data Transformation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467b389",
   "metadata": {},
   "source": [
    "## Pipeline is built using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2260d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe543fd",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361319dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_pipeline(data):\n",
    "    \n",
    "    # Identifying types of features/datatypes of columns\n",
    "    numeric_features = data.select_dtypes(include=['float', 'int']).columns\n",
    "    categorical_features = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Handling missing values in numeric features\n",
    "    data[numeric_features] = data[numeric_features].fillna(data[numeric_features].mean())\n",
    "    \n",
    "    # Handling missing values in categorical features\n",
    "    data[categorical_features] = data[categorical_features].fillna(data[categorical_features].mode().iloc[0])\n",
    "    \n",
    "    \n",
    "    # Handling outliers in numeric features\n",
    "    for feature in numeric_features:\n",
    "        \n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        \n",
    "        data[feature] = np.where((data[feature] < lower_bound) | (data[feature] > upper_bound)\n",
    "                                 ,data[feature].mean()\n",
    "                                 ,data[feature])\n",
    "    \n",
    "    \n",
    "    # Normalizing numeric features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data[numeric_features])\n",
    "    data[numeric_features] = scaler.transform(data[numeric_features])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea34afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c7cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: \n",
      "   NumericFeature1  NumericFeature2 CategoricalFeature\n",
      "0              1.0                7                  A\n",
      "1              2.0                8                  B\n",
      "2              NaN                9                NaN\n",
      "3              4.0               10                  A\n",
      "4              5.0               11                  B\n",
      "5              6.0               50                  C\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data: \")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb42989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "transformed_data = data_preprocessing_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2816bfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data: \n",
      "   NumericFeature1  NumericFeature2 CategoricalFeature\n",
      "0        -1.535624        -1.099370                  A\n",
      "1        -0.944999        -0.749128                  B\n",
      "2         0.000000        -0.398886                  A\n",
      "3         0.236250        -0.048645                  A\n",
      "4         0.826874         0.301597                  B\n",
      "5         1.417499         1.994431                  C\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Data: \")\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c288a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data Preprocessing involves transforming and manipulating raw data to improve its quality, consistency, and relevance for analysis. A data preprocessing pipeline is a systematic and automated approach that combines multiple preprocessing steps into a cohesive workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d8e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
